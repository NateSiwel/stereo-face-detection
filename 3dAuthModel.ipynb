{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import pickle\n",
    "from server import server, app, decode_img, list_to_numpy\n",
    "from serverClass import ServerClass\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "import face_recognition\n",
    "\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('test_data.pickle', 'rb') as handle:\n",
    "    data = pickle.load(handle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [],
   "source": [
    "server = ServerClass()\n",
    "\n",
    "imgsL = []\n",
    "imgsR = []\n",
    "cam = list_to_numpy(data[0]['cam'])\n",
    "\n",
    "for obj in data:\n",
    "  imgL = obj['imgL']\n",
    "  imgR = obj['imgR'] \n",
    "  #cam = obj['cam']\n",
    "\n",
    "  imgL = decode_img(imgL)\n",
    "  imgR = decode_img(imgR)\n",
    "\n",
    "  imgsL.append(imgL)\n",
    "  imgsR.append(imgR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [],
   "source": [
    "imgsL_rect, imgsR_rect = [], []\n",
    "for imgL, imgR in zip(imgsL, imgsR):\n",
    "  imgL_rect, imgR_rect = server.rectify_frames(imgL, imgR, cam)\n",
    "  imgsL_rect.append(imgL_rect)\n",
    "  imgsR_rect.append(imgR_rect)\n",
    "\n",
    "#imgL_rect, imgR_rect = server.rectify_frames(imgL, imgR, cam)\n",
    "\n",
    "rectsL_list=[]\n",
    "for imgL_rect in imgsL_rect:\n",
    "  rectsL_list.append(face_recognition.face_locations(imgL_rect))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "300\n"
     ]
    }
   ],
   "source": [
    "print(len(imgsL_rect))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [],
   "source": [
    "disparity_maps = []\n",
    "for imgL_rect, imgR_rect in zip(imgsL_rect, imgsR_rect):\n",
    "    #for top, right, bottom, left in rects:\n",
    "        #cv2.rectangle(imgL_rect, (left, top), (right, bottom), (0, 0, 255), 2)\n",
    "    disparity_map = server.get_disparity(\n",
    "        img1_rectified=imgL_rect, \n",
    "        img2_rectified=imgR_rect, \n",
    "        min_disp=-100, \n",
    "        max_disp=100, \n",
    "        block_size=1, \n",
    "        uniquenessRatio=0, \n",
    "        speckleWindowSize=0, \n",
    "        speckleRange=1, \n",
    "        disp12MaxDiff=0\n",
    "    )\n",
    "    disparity_maps.append(disparity_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nfor testing get_disparity parameters \\n\\n\\nindex = 25\\ntest_map = server.get_disparity(\\n        img1_rectified=imgsL_rect[index], \\n        img2_rectified=imgsR_rect[index], \\n        min_disp=-100, \\n        max_disp=100, \\n        block_size=1, \\n        uniquenessRatio=0, \\n        speckleWindowSize=0, \\n        speckleRange=1, \\n        disp12MaxDiff=0\\n    )\\n\\nfor disparity_map in disparity_maps:\\n    plt.imshow(disparity_map, cmap='gray')\\n    plt.title('Disparity Map')\\n    plt.colorbar()\\n    plt.show()\\n\\nplt.imshow(disparity_map, cmap='gray')\\nplt.title('Disparity Map')\\nplt.colorbar()\\nplt.show()\\n\\ncv2.imshow('img1', imgL)\\ncv2.waitKey(0)\\ncv2.destroyAllWindows()\\n\""
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "for testing get_disparity parameters \n",
    "\n",
    "index = 25\n",
    "test_map = server.get_disparity(\n",
    "        img1_rectified=imgsL_rect[index], \n",
    "        img2_rectified=imgsR_rect[index], \n",
    "        min_disp=-100, \n",
    "        max_disp=100, \n",
    "        block_size=1, \n",
    "        uniquenessRatio=0, \n",
    "        speckleWindowSize=0, \n",
    "        speckleRange=1, \n",
    "        disp12MaxDiff=0\n",
    "    )\n",
    "\n",
    "plt.imshow(test_map, cmap='gray')\n",
    "plt.title('Disparity Map')\n",
    "plt.colorbar()\n",
    "plt.show()\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "300\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n",
      "22\n",
      "23\n",
      "24\n",
      "25\n",
      "26\n",
      "27\n",
      "28\n",
      "29\n",
      "30\n",
      "31\n",
      "32\n",
      "33\n",
      "34\n",
      "35\n",
      "36\n",
      "37\n",
      "38\n",
      "39\n",
      "41\n",
      "43\n",
      "44\n",
      "45\n",
      "46\n",
      "47\n",
      "49\n",
      "50\n",
      "51\n",
      "52\n",
      "53\n",
      "54\n",
      "55\n",
      "56\n",
      "57\n",
      "58\n",
      "59\n",
      "60\n",
      "61\n",
      "62\n",
      "63\n",
      "64\n",
      "65\n",
      "66\n",
      "67\n",
      "68\n",
      "69\n",
      "70\n",
      "71\n",
      "72\n",
      "73\n",
      "74\n",
      "75\n",
      "76\n",
      "77\n",
      "78\n",
      "81\n",
      "82\n",
      "85\n",
      "86\n",
      "87\n",
      "89\n",
      "90\n",
      "91\n",
      "92\n",
      "93\n",
      "94\n",
      "95\n",
      "96\n",
      "97\n",
      "98\n",
      "99\n",
      "100\n",
      "101\n",
      "102\n",
      "103\n",
      "105\n",
      "106\n",
      "107\n",
      "109\n",
      "111\n",
      "112\n",
      "113\n",
      "115\n",
      "116\n",
      "117\n",
      "118\n",
      "119\n",
      "120\n",
      "121\n",
      "122\n",
      "123\n",
      "124\n",
      "125\n",
      "126\n",
      "127\n",
      "128\n",
      "129\n",
      "130\n",
      "131\n",
      "132\n",
      "135\n",
      "136\n",
      "137\n",
      "138\n",
      "139\n",
      "140\n",
      "142\n",
      "143\n",
      "144\n",
      "145\n",
      "146\n",
      "147\n",
      "148\n",
      "149\n",
      "150\n",
      "151\n",
      "152\n",
      "153\n",
      "154\n",
      "155\n",
      "156\n",
      "157\n",
      "158\n",
      "159\n",
      "160\n",
      "161\n",
      "162\n",
      "163\n",
      "164\n",
      "165\n",
      "166\n",
      "167\n",
      "168\n",
      "169\n",
      "170\n",
      "171\n",
      "172\n",
      "173\n",
      "174\n",
      "175\n",
      "176\n",
      "177\n",
      "178\n",
      "179\n",
      "180\n",
      "181\n",
      "182\n",
      "183\n",
      "184\n",
      "185\n",
      "186\n",
      "187\n",
      "188\n",
      "189\n",
      "190\n",
      "191\n",
      "192\n",
      "193\n",
      "194\n",
      "195\n",
      "196\n",
      "197\n",
      "198\n",
      "199\n",
      "200\n",
      "201\n",
      "203\n",
      "204\n",
      "205\n",
      "206\n",
      "207\n",
      "208\n",
      "209\n",
      "210\n",
      "211\n",
      "212\n",
      "213\n",
      "214\n",
      "215\n",
      "216\n",
      "217\n",
      "218\n",
      "219\n",
      "220\n",
      "221\n",
      "222\n",
      "223\n",
      "223\n",
      "224\n",
      "225\n",
      "226\n",
      "227\n",
      "228\n",
      "229\n",
      "230\n",
      "231\n",
      "232\n",
      "233\n",
      "234\n",
      "235\n",
      "236\n",
      "237\n",
      "238\n",
      "239\n",
      "240\n",
      "241\n",
      "242\n",
      "243\n",
      "244\n",
      "245\n",
      "246\n",
      "247\n",
      "248\n",
      "249\n",
      "250\n",
      "251\n",
      "252\n",
      "253\n",
      "258\n",
      "259\n",
      "260\n",
      "260\n",
      "261\n",
      "262\n",
      "263\n",
      "264\n",
      "265\n",
      "266\n",
      "267\n",
      "268\n",
      "269\n",
      "270\n",
      "271\n",
      "272\n",
      "273\n",
      "274\n",
      "275\n",
      "276\n",
      "277\n",
      "277\n",
      "278\n",
      "279\n",
      "280\n",
      "281\n",
      "283\n",
      "286\n",
      "287\n",
      "288\n",
      "290\n",
      "291\n",
      "292\n",
      "293\n",
      "294\n",
      "295\n",
      "297\n",
      "298\n",
      "299\n"
     ]
    }
   ],
   "source": [
    "disparity_faces = []\n",
    "padding_percentage = 0.20\n",
    "target_size = (224,224)\n",
    "\n",
    "print(len(rectsL_list[20]))\n",
    "print(len(disparity_maps))\n",
    "\n",
    "for i, rectsL in enumerate(rectsL_list):\n",
    "    height, width = disparity_maps[i].shape[:2]  # Get the dimensions of the disparity map\n",
    "    face_region=None\n",
    "\n",
    "    for x, (top, right, bottom, left) in enumerate(rectsL):\n",
    "\n",
    "        # Calculate dynamic padding based on the size of the face\n",
    "        pad_w = int((right - left) * padding_percentage)\n",
    "        pad_h = int((bottom - top) * padding_percentage)\n",
    "        \n",
    "        # Apply padding and ensure indices are within the image bounds\n",
    "        padded_top = max(top - pad_h, 0)\n",
    "        padded_bottom = min(bottom + pad_h, height)\n",
    "        padded_left = max(left - pad_w, 0)\n",
    "        padded_right = min(right + pad_w, width)\n",
    "        \n",
    "        # Extract the face region with padding\n",
    "        face_region = disparity_maps[i][padded_top:padded_bottom, padded_left:padded_right]\n",
    "        face_normal = imgsL_rect[i][padded_top:padded_bottom, padded_left:padded_right]\n",
    "\n",
    "        face_region_resized = cv2.resize(face_region, target_size, interpolation=cv2.INTER_AREA)\n",
    "        face_normal_resized = cv2.resize(face_normal, target_size, interpolation=cv2.INTER_AREA)\n",
    "\n",
    "        \n",
    "        \"\"\"\n",
    "        if i == 20:\n",
    "            cv2.imshow('face', imgsL_rect[i])\n",
    "            cv2.waitKey(0)\n",
    "            cv2.destroyAllWindows()\n",
    "        \"\"\"\n",
    "\n",
    "        print(i)\n",
    "\n",
    "    disparity_faces.append(face_region_resized)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nfor i, (top, right, bottom, left) in enumerate(rectsL):\\n    # Draw rectangle on the disparity map\\n    cv2.rectangle(disparity_map, (left, top), (right, bottom), (255, 0, 0), 2)\\n\\ncv2.imshow('Disparity Map with Face Regions', disparity_map)\\ncv2.waitKey(0)  # Wait for a key press to close\\ncv2.destroyAllWindows()\\n\""
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "for i, (top, right, bottom, left) in enumerate(rectsL):\n",
    "    # Draw rectangle on the disparity map\n",
    "    cv2.rectangle(disparity_map, (left, top), (right, bottom), (255, 0, 0), 2)\n",
    "\n",
    "cv2.imshow('Disparity Map with Face Regions', disparity_map)\n",
    "cv2.waitKey(0)  # Wait for a key press to close\n",
    "cv2.destroyAllWindows()\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nfor face in disparity_faces:\\n  plt.figure(figsize=(10, 7))\\n  plt.imshow(face, cmap='gray')\\n  plt.title('Disparity Map')\\n  plt.colorbar()  # Adds a colorbar to interpret the values\\n  plt.show()\\n\""
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Display the disparity map using matplotlib\n",
    "\n",
    "\"\"\"\n",
    "for face in disparity_faces:\n",
    "  plt.figure(figsize=(10, 7))\n",
    "  plt.imshow(face, cmap='gray')\n",
    "  plt.title('Disparity Map')\n",
    "  plt.colorbar()  # Adds a colorbar to interpret the values\n",
    "  plt.show()\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [],
   "source": [
    "index = 151\n",
    "plt.figure(figsize=(10, 7))\n",
    "plt.imshow(disparity_maps[index], cmap='gray')\n",
    "plt.title('Disparity Map')\n",
    "plt.colorbar()  # Adds a colorbar to interpret the values\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = []\n",
    "y_train = []\n",
    "for i, face in enumerate(disparity_faces):\n",
    "  if face is not None:\n",
    "    X_train.append(face)\n",
    "\n",
    "    #1 for valid face, 0 for invalid face\n",
    "    if i > 150:\n",
    "      y_train.append(0)\n",
    "    else:\n",
    "      y_train.append(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "class SimpleCNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SimpleCNN, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 16, kernel_size=3, stride=1, padding=1)\n",
    "        self.act1 = nn.ReLU()\n",
    "        self.pool1 = nn.MaxPool2d(kernel_size=2)\n",
    "        self.conv2 = nn.Conv2d(16, 32, kernel_size=3, stride=1, padding=1)\n",
    "        self.act2 = nn.ReLU()\n",
    "        self.pool2 = nn.MaxPool2d(kernel_size=2)\n",
    "        self.fc1 = nn.Linear(32 * 56 * 56, 128)\n",
    "        self.act3 = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(128, 2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool1(self.act1(self.conv1(x)))\n",
    "        x = self.pool2(self.act2(self.conv2(x)))\n",
    "        x = torch.flatten(x, 1) \n",
    "        x = self.act3(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([300, 1, 224, 224])\n"
     ]
    }
   ],
   "source": [
    "X_train_tensor = torch.tensor(X_train, dtype=torch.float32).unsqueeze(1)\n",
    "print(X_train_tensor.shape)\n",
    "y_train_tensor = torch.tensor(y_train, dtype=torch.long)\n",
    "\n",
    "dataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
    "train_loader = DataLoader(dataset, batch_size=16, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss: 163.677799\n",
      "Epoch 2, Loss: 2.686531\n",
      "Epoch 3, Loss: 1.242428\n",
      "Epoch 4, Loss: 0.237460\n",
      "Epoch 5, Loss: 0.191915\n",
      "Epoch 6, Loss: 0.113278\n",
      "Epoch 7, Loss: 0.040785\n",
      "Epoch 8, Loss: 0.000001\n",
      "Epoch 9, Loss: 0.000264\n",
      "Epoch 10, Loss: 0.000000\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = SimpleCNN().to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Training loop\n",
    "num_epochs = 10\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    for inputs, labels in train_loader:\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "\n",
    "    print(f\"Epoch {epoch+1}, Loss: {running_loss/len(train_loader):.6f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 1, 224, 224])\n",
      "Predicted class: 1\n",
      "Probabilities: tensor([[0., 1.]], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "# set model to eval mode\n",
    "model.eval()\n",
    "\n",
    "#prep input image\n",
    "test_image = torch.tensor(X_train[22], dtype=torch.float32).unsqueeze(0)\n",
    "test_image = test_image.unsqueeze(1)\n",
    "test_image = test_image.to(device) \n",
    "print(test_image.shape)\n",
    "\n",
    "with torch.no_grad(): \n",
    "    output = model(test_image)\n",
    "\n",
    "probabilities = torch.nn.functional.softmax(output, dim=1)\n",
    "predicted_class = torch.argmax(probabilities, dim=1)\n",
    "\n",
    "print(f\"Predicted class: {predicted_class.item()}\")\n",
    "print(f\"Probabilities: {probabilities}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), 'auth.pth')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cuda-testing",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
